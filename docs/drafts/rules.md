Rules
ğŸ“Œ Vocabulary JSON Placement â€” Canonical Rule

1. ar_u_roots
	â€¢	âŒ NO vocabulary meanings
	â€¢	âŒ NO cards
	â€¢	âœ” Root identity only

2. ar_u_tokens
	â€¢	âœ” Basic vocabulary (lemma-level)
	â€¢	âœ” One simple meaning set
	â€¢	âœ” Optional basic card

3. ar_u_lexicon
	â€¢	âœ” Full meaning ranges
	â€¢	âœ” Sense distinctions
	â€¢	âœ” Qurâ€™anic / contextual nuance
	â€¢	âœ” Proper vocab cards

â¸»

ğŸ“Œ Card Rule (Minimal)
	â€¢	Front â†’ unique occurrence (lemma + ref)
	â€¢	Back â†’ range of meanings

â¸»

ğŸ“Œ Law (One Line)
Roots define origin, tokens define words, lexicon defines meaning.

Here is the minimal, clean DOs & DONâ€™Ts for your system structure.

â¸»
## Roots
âœ… DOs
	â€¢	Build bottom-up
Universal â†’ Occurrence â†’ Container/Lesson
	â€¢	Use SHA only for universal IDs
From canonical_input only
	â€¢	Keep roots pure
Identity only, no meanings
	â€¢	Put vocabulary on tokens / lexicon
Not on roots
	â€¢	Normalize text only for search
Store normalized fields separately
	â€¢	Reuse universals everywhere
One root / token / span forever
	â€¢	Use spans for grouping, sentences for predication

â¸»

âŒ DONâ€™Ts
	â€¢	âŒ Donâ€™t add meanings to ar_u_roots
	â€¢	âŒ Donâ€™t hash normalized/search text
	â€¢	âŒ Donâ€™t create universals from occurrences
	â€¢	âŒ Donâ€™t mix container logic with semantics
	â€¢	âŒ Donâ€™t duplicate tokens, spans, or sentences
	â€¢	âŒ Donâ€™t treat verb+prep as a noun span
	â€¢	âŒ Donâ€™t analyze before structure exists

â¸»

âš–ï¸ Law (One Line)

Identity is universal, location is occurrence, presentation is container.

Thatâ€™s the final structure rule set.